{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "I was really intrigued by watching youtube videos of people making game-playing AI, and I really wanted to learn it. Follow me as I make use of OpenAI's gymnasium library to train a deep reinforcement learning model to balance a pole on a moving cart."
      ],
      "metadata": {
        "id": "wQmp-Xy_OwQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium\n",
        "!pip install swig\n",
        "!pip install gymnasium[box2d]\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "V-kc6OoxXyQ5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Key Concepts in Reinforcement Learning (RL)\n",
        "1. Agent and Environment: The \"agent\" is the AI trying to learn, while the \"environment\" is the task or game it interacts with. For example, in the \"CartPole\" game, the agent tries to balance a pole on a moving cart.\n",
        "\n",
        "2. State: This represents the current situation of the environment. It can be a set of values, like the position and angle of the pole.\n",
        "\n",
        "3. Action: This is a move or choice the agent can make (e.g., moving the cart left or right).\n",
        "\n",
        "4. Reward: A number given to the agent to indicate how well it is doing. The goal of the agent is to maximize its total reward over time.\n",
        "\n",
        "5. Q-Value: The \"quality\" of a specific action in a given state, representing the expected future reward. Higher Q-values indicate better actions. More about this comes later in the notebook."
      ],
      "metadata": {
        "id": "ytabQWKqY7_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setting Up the Environment\n",
        "\n",
        "1. What is Gymnasium? It’s a toolkit that helps you create and manage environments for RL. Think of an \"environment\" as a game or task that an AI (or agent) will learn to complete.\n",
        "\n",
        "2. Choosing an Environment: Gymnasium has various environments (e.g., CartPole, MountainCar). Each one represents a different problem for the agent to solve, like balancing a pole or climbing a hill."
      ],
      "metadata": {
        "id": "pyFNx5HGOjrF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports -\n",
        "gymnasium: Contains the prebuilt environments (e.g., CartPole).\n",
        "\n",
        "numpy: For numerical operations.\n",
        "\n",
        "random: Helps with action exploration.\n",
        "\n",
        "torch: PyTorch library for building and training the neural network.\n",
        "\n",
        "deque: Efficiently stores past experiences for replaying during training."
      ],
      "metadata": {
        "id": "96aSGxXPRQs0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eVWXxDTI6273",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e0e6583-7e2b-4737-e007-7cd8ba3d8939"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# Example environment setup\n",
        "env = gym.make('CartPole-v1')  # Set render_mode to \"human\" for real-time rendering"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hyperparameters\n",
        "GAMMA = 0.99: Gamma is the discount factor, controlling how much the agent\n",
        "cares about future rewards versus immediate rewards. A value close to 1 means it values long-term rewards.\n",
        "\n",
        "LEARNING_RATE = 0.001: This rate tells the optimizer how much to adjust the neural network’s weights with each step. Small changes (e.g., 0.001) are typical for stability.\n",
        "\n",
        "BATCH_SIZE = 64: When training the network, it learns from a group of experiences, called a \"batch,\" which in this case contains 64 experiences.\n",
        "\n",
        "REPLAY_BUFFER_SIZE = 10000: This is the maximum number of experiences stored in memory. Old experiences are discarded when the buffer is full.\n",
        "\n",
        "TARGET_UPDATE_FREQ = 10: The \"target network\" (a stable copy of our network used for learning) is updated every 10 episodes to prevent rapid changes.\n",
        "\n",
        "EPSILON_START = 1.0: Epsilon controls how much the agent explores randomly. It starts at 1 (fully random).\n",
        "\n",
        "EPSILON_MIN = 0.01: Epsilon’s minimum value. The agent will always explore a tiny bit, even after learning.\n",
        "\n",
        "EPSILON_DECAY = 0.995: Epsilon decreases by multiplying by this value each episode, moving gradually from exploration to exploitation.\n",
        "\n",
        "NUM_EPISODES = 500: The total number of episodes the agent will train.\n",
        "\n",
        "MAX_STEPS = 200: The maximum steps the agent takes within an episode before it stops."
      ],
      "metadata": {
        "id": "UU87_i6DZggP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "GAMMA = 0.99             # Discount factor for future rewards\n",
        "LEARNING_RATE = 0.001    # Learning rate for the optimizer\n",
        "BATCH_SIZE = 64          # Number of experiences sampled for each training step\n",
        "REPLAY_BUFFER_SIZE = 10000 # Max size of the replay buffer\n",
        "TARGET_UPDATE_FREQ = 10  # How often to update the target network\n",
        "EPSILON_START = 1.0      # Initial exploration rate\n",
        "EPSILON_MIN = 0.01       # Minimum exploration rate\n",
        "EPSILON_DECAY = 0.995    # Rate of decay for exploration rate\n",
        "NUM_EPISODES = 500       # Total number of training episodes\n",
        "MAX_STEPS = 200          # Max steps per episode\n"
      ],
      "metadata": {
        "id": "osNNFl9GRpLN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Understanding the Environment\n",
        "1. Observation Space: The environment gives observations, which describe the state of the task. For example, in CartPole, observations include the cart’s position, the pole’s angle, etc.\n",
        "\n",
        "2. Action Space: This is the set of possible actions the agent can take, like moving left or right. The agent will pick actions based on what it \"sees\" in the observation space."
      ],
      "metadata": {
        "id": "A3S3UhyqPU2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get observation and action spaces\n",
        "print(env.observation_space)  # Example: position and velocity of CartPole\n",
        "print(env.action_space)       # Example: moving left or right"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLJWtHg6PQJ5",
        "outputId": "e7086c3b-45e6-4b29-8db7-b2ed462083dc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
            "Discrete(2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Defining the Agent\n",
        "1. Agent’s Goal: The agent’s goal is to maximize a reward. It learns to take actions that increase its reward over time. This is where deep learning comes in—usually, a neural network decides which actions to take. In simpler environments, a Q-table is used, where all the possible states have a Q value. It could be used here as well, but I want to overkill. Here is the formula for calculating those taken straight from Wikipedia (https://en.wikipedia.org/wiki/Q-learning) -\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "\n",
        "2. Neural Network: The neural network takes in the observation as input and outputs a probability or value for each action. With enough training, it \"learns\" which actions work best in different situations."
      ],
      "metadata": {
        "id": "pzHERl6mPstP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Define the Q-Network\n",
        "The Q-Network takes the current state as input and outputs Q-values for each action.\n",
        "\n",
        "fc1, fc2, fc3: Three fully connected layers with ReLU activations between them. The final layer outputs a value for each action (e.g., left or right for CartPole).\n",
        "\n",
        "1. class QNetwork(nn.Module):: We define a neural network class called QNetwork, inheriting from nn.Module (a PyTorch base class for models).\n",
        "\n",
        "2. def __init__(self, state_dim, action_dim):: This constructor method initializes the network with inputs for the state and action sizes.\n",
        "\n",
        "3. state_dim: The number of values that represent the state (e.g., position and velocity in CartPole).\n",
        "action_dim: The number of possible actions (e.g., left or right for CartPole).\n",
        "super(QNetwork, self).__init__(): Calls the parent __init__ method from nn.Module.\n",
        "\n",
        "4. Network Layers:\n",
        "\n",
        "* self.fc1 = nn.Linear(state_dim, 128): The first fully connected (fc) layer, which takes in the state and outputs 128 values. nn.Linear creates a layer with weights for learning.\n",
        "\n",
        "* self.fc2 = nn.Linear(128, 64): The second layer, taking 128 inputs from the\n",
        "previous layer and outputting 64.\n",
        "\n",
        "* self.fc3 = nn.Linear(64, action_dim): The final layer outputs values equal to the number of actions, representing the Q-values for each action.\n",
        "\n",
        "5. def forward(self, x):: This function processes the input through the network.\n",
        "\n",
        "* x = torch.relu(self.fc1(x)): The input goes through fc1 and then relu, an activation function that adds non-linearity, allowing the network to learn complex patterns.\n",
        "\n",
        "* x = torch.relu(self.fc2(x)): Passes through fc2 and relu.\n",
        "\n",
        "* return self.fc3(x): The final layer outputs Q-values for each action."
      ],
      "metadata": {
        "id": "sgsbxnhXR5Kv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ],
      "metadata": {
        "id": "vdLXQY5XPrlj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the Experience Replay Buffer\n",
        "The ReplayBuffer stores past experiences (state, action, reward, next_state, done), which helps in stabilizing training by breaking correlations."
      ],
      "metadata": {
        "id": "YfRhjFI3SHhU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. class ReplayBuffer:: This is a class definition for the replay buffer.\n",
        "\n",
        "2. def __init__(self, capacity):: The __init__ function initializes the buffer with a fixed capacity, the maximum number of experiences it can hold.\n",
        "\n",
        "* self.buffer = deque(maxlen=capacity): deque is a data structure that works like a list but is optimized for fast addition/removal at both ends. Setting maxlen means that once the buffer reaches the capacity limit, the oldest experience will automatically be removed.\n",
        "3. def push(self, experience):: This function adds an experience to the buffer.\n",
        "\n",
        "* self.buffer.append(experience): Appends a new experience to buffer. An experience is a tuple containing the state, action, reward, next state, and done flag (indicating if the episode has ended).\n",
        "4. def sample(self, batch_size):: This function randomly samples a batch of experiences from the buffer.\n",
        "\n",
        "* return random.sample(self.buffer, batch_size): The random.sample function picks batch_size elements randomly, helping the agent to learn from diverse past experiences.\n",
        "5. def __len__(self):: This function returns the number of items in the buffer.\n",
        "\n",
        "* return len(self.buffer): Allows us to check if the buffer has enough experiences to train the model (at least BATCH_SIZE items)."
      ],
      "metadata": {
        "id": "3rlPPetnsNoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "id": "70yg79JQSGaa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training the Agent with Deep RL\n",
        "1. Exploration vs. Exploitation: At first, the agent explores by trying random actions to gather information. Over time, it begins to exploit its knowledge by picking actions that give high rewards.\n",
        "\n",
        "2. Rewards and Learning: The agent learns by updating the network based on rewards. Algorithms like Q-learning, DQN, and PPO are used to adjust the network to predict or maximize future rewards."
      ],
      "metadata": {
        "id": "RYXiOBorQ1ld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Select Action Using an Epsilon-Greedy Policy\n",
        "The agent chooses actions using an epsilon-greedy policy: with probability epsilon, it picks a random action (exploration); otherwise, it picks the best-known action (exploitation)."
      ],
      "metadata": {
        "id": "Z_gHnBj_SYIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_action(state, policy_net, epsilon):\n",
        "    if random.random() < epsilon:\n",
        "        return env.action_space.sample()  # Explore\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            return policy_net(state).argmax().item()  # Exploit\n"
      ],
      "metadata": {
        "id": "wEdfdP8EQ8sr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. def select_action(state, policy_net, epsilon):: This function decides which action the agent should take based on the current state, the neural network (policy_net), and the exploration rate epsilon.\n",
        "\n",
        "2. if random.random() < epsilon:: random.random() generates a random number between 0 and 1. If it’s less than epsilon, the agent will explore (choose a random action).\n",
        "\n",
        "3. return env.action_space.sample(): Calls the environment’s sample() function to pick a random action from the available actions.\n",
        "else:: If random.random() is not less than epsilon, the agent will exploit, choosing the action that gives the highest Q-value according to policy_net.\n",
        "\n",
        "4. with torch.no_grad():: Temporarily turns off gradient tracking in PyTorch, saving memory and computation. This is because we don’t need to update the network during action selection.\n",
        "\n",
        "5. return policy_net(torch.tensor(state, dtype=torch.float32)).argmax().item(): Converts state to a tensor and feeds it into policy_net. The network outputs Q-values for all actions, and argmax() picks the action with the highest Q-value."
      ],
      "metadata": {
        "id": "cHpQaY6TtELG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Optimize the model\n",
        "This function updates the neural network based on a batch of experiences, adjusting weights to minimize the error between predicted and actual Q-values. This process is called backpropagation.\n",
        "\n",
        "MSE Loss: Measures the difference between the expected Q-values and the predicted Q-values.\n",
        "\n",
        "Optimizer Step: Updates the neural network weights to minimize the loss."
      ],
      "metadata": {
        "id": "Y1wqromOSl_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_model():\n",
        "    if len(replay_buffer) < BATCH_SIZE:\n",
        "        return\n",
        "\n",
        "    # Sample a batch and convert lists of arrays to numpy arrays for efficient tensor conversion\n",
        "    batch = replay_buffer.sample(BATCH_SIZE)\n",
        "    states, actions, rewards, next_states, dones = map(np.array, zip(*batch))  # Converts each to a numpy array\n",
        "\n",
        "    # Convert numpy arrays to PyTorch tensors and move to device\n",
        "    states = torch.tensor(states, dtype=torch.float32).to(device)\n",
        "    actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1).to(device)\n",
        "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "    next_states = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
        "    dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Calculate Q values and optimize\n",
        "    current_q_values = policy_net(states).gather(1, actions).squeeze()\n",
        "    next_q_values = target_net(next_states).max(1)[0]\n",
        "    expected_q_values = rewards + (GAMMA * next_q_values * (1 - dones))\n",
        "\n",
        "    loss = nn.MSELoss()(current_q_values, expected_q_values)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "77_BXzBvSspp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "def optimize_model():: This function optimizes (or trains) the neural network using a sample of experiences.\n",
        "\n",
        "if len(replay_buffer) < BATCH_SIZE:: Checks if there are enough experiences in the replay_buffer for training. If not, it returns without training.\n",
        "\n",
        "batch = replay_buffer.sample(BATCH_SIZE): Gets a random batch of experiences from the buffer.\n",
        "\n",
        "states, actions, rewards, next_states, dones = zip(*batch): Unpacks the batch into separate lists for states, actions, rewards, next_states, and dones.\n",
        "\n",
        "Convert to PyTorch tensors: Converts each list into a tensor format PyTorch can process.\n",
        "\n",
        ".unsqueeze(1): Adds an extra dimension to actions to match PyTorch’s expected input shape.\n",
        "Calculate the Q values:\n",
        "\n",
        "current_q_values = policy_net(states).gather(1, actions).squeeze(): Predicts Q-values for the states and selects Q-values for the specific actions taken by the agent. .squeeze() removes unnecessary dimensions.\n",
        "\n",
        "next_q_values = target_net(next_states).max(1)[0]: Uses the target_net to predict the Q-values for next_states and picks the maximum value (best action in the next state).\n",
        "\n",
        "expected_q_values = rewards + (GAMMA * next_q_values * (1 - dones)): This is the Bellman equation. It calculates the expected Q-value by adding the reward to the discounted (GAMMA) future Q-value of the next state. The term (1 - dones) ensures that if the episode is over, the future Q-value is zero.\n",
        "\n",
        "Calculate loss and optimize:\n",
        "\n",
        "loss = nn.MSELoss()(current_q_values, expected_q_values): Measures the difference between the predicted Q-values and the expected Q-values using Mean Squared Error (MSE), which calculates the average of the squared differences.\n",
        "\n",
        "optimizer.zero_grad(): Clears any previous gradients, resetting them before backpropagation.\n",
        "\n",
        "loss.backward(): Calculates the gradients (partial derivatives) of the loss with respect to each weight in the network.\n",
        "\n",
        "optimizer.step(): Updates the weights in the neural network to minimize the loss, moving closer to the expected Q-values.\n",
        "\n"
      ],
      "metadata": {
        "id": "QOd11t85trp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training Loop\n",
        "1. Environment Reset: The environment is reset at the beginning of each episode.\n",
        "2. Action Selection: Choose an action based on the current state and epsilon-greedy policy.\n",
        "3. Step Through Environment: Take the action, observe the next state, reward, and done signal (if the episode is over).\n",
        "4. Store Experience: Add the experience tuple to the replay buffer.\n",
        "5. Optimize Model: If enough experiences are in the replay buffer, sample a batch and train the model.\n",
        "6. Epsilon Decay: Reduce exploration over time as the agent learns.\n",
        "7. Target Network Update: Periodically update the target network with the policy network’s weights."
      ],
      "metadata": {
        "id": "Kap7_AqESvTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#env = gym.make('CartPole-v1')\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "policy_net = QNetwork(state_dim, action_dim).to(device)\n",
        "target_net = QNetwork(state_dim, action_dim).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())  # Copy weights from policy network\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
        "replay_buffer = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
        "\n",
        "epsilon = EPSILON_START\n",
        "\n",
        "for episode in range(NUM_EPISODES):\n",
        "    state = torch.tensor(env.reset()[0], dtype=torch.float32).to(device)\n",
        "    total_reward = 0\n",
        "\n",
        "    for t in range(MAX_STEPS):\n",
        "        action = select_action(state, policy_net, epsilon)\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        next_state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
        "\n",
        "        # Push experience to replay buffer (as numpy for CPU storage)\n",
        "        replay_buffer.push((state.cpu().numpy(), action, reward, next_state.cpu().numpy(), done))\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        optimize_model()  # Train the network\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    epsilon = max(EPSILON_MIN, epsilon * EPSILON_DECAY)\n",
        "\n",
        "    if episode % TARGET_UPDATE_FREQ == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnJercZeS8Fj",
        "outputId": "4048cebc-863f-41a8-e714-02c68ff88be3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: Total Reward = 26.0\n",
            "Episode 2: Total Reward = 24.0\n",
            "Episode 3: Total Reward = 18.0\n",
            "Episode 4: Total Reward = 14.0\n",
            "Episode 5: Total Reward = 13.0\n",
            "Episode 6: Total Reward = 13.0\n",
            "Episode 7: Total Reward = 47.0\n",
            "Episode 8: Total Reward = 19.0\n",
            "Episode 9: Total Reward = 18.0\n",
            "Episode 10: Total Reward = 19.0\n",
            "Episode 11: Total Reward = 13.0\n",
            "Episode 12: Total Reward = 31.0\n",
            "Episode 13: Total Reward = 20.0\n",
            "Episode 14: Total Reward = 20.0\n",
            "Episode 15: Total Reward = 17.0\n",
            "Episode 16: Total Reward = 11.0\n",
            "Episode 17: Total Reward = 17.0\n",
            "Episode 18: Total Reward = 28.0\n",
            "Episode 19: Total Reward = 12.0\n",
            "Episode 20: Total Reward = 21.0\n",
            "Episode 21: Total Reward = 15.0\n",
            "Episode 22: Total Reward = 60.0\n",
            "Episode 23: Total Reward = 22.0\n",
            "Episode 24: Total Reward = 15.0\n",
            "Episode 25: Total Reward = 9.0\n",
            "Episode 26: Total Reward = 19.0\n",
            "Episode 27: Total Reward = 16.0\n",
            "Episode 28: Total Reward = 60.0\n",
            "Episode 29: Total Reward = 24.0\n",
            "Episode 30: Total Reward = 57.0\n",
            "Episode 31: Total Reward = 21.0\n",
            "Episode 32: Total Reward = 12.0\n",
            "Episode 33: Total Reward = 12.0\n",
            "Episode 34: Total Reward = 51.0\n",
            "Episode 35: Total Reward = 22.0\n",
            "Episode 36: Total Reward = 16.0\n",
            "Episode 37: Total Reward = 35.0\n",
            "Episode 38: Total Reward = 72.0\n",
            "Episode 39: Total Reward = 36.0\n",
            "Episode 40: Total Reward = 23.0\n",
            "Episode 41: Total Reward = 55.0\n",
            "Episode 42: Total Reward = 16.0\n",
            "Episode 43: Total Reward = 100.0\n",
            "Episode 44: Total Reward = 82.0\n",
            "Episode 45: Total Reward = 104.0\n",
            "Episode 46: Total Reward = 38.0\n",
            "Episode 47: Total Reward = 25.0\n",
            "Episode 48: Total Reward = 24.0\n",
            "Episode 49: Total Reward = 14.0\n",
            "Episode 50: Total Reward = 46.0\n",
            "Episode 51: Total Reward = 35.0\n",
            "Episode 52: Total Reward = 41.0\n",
            "Episode 53: Total Reward = 35.0\n",
            "Episode 54: Total Reward = 14.0\n",
            "Episode 55: Total Reward = 12.0\n",
            "Episode 56: Total Reward = 44.0\n",
            "Episode 57: Total Reward = 45.0\n",
            "Episode 58: Total Reward = 17.0\n",
            "Episode 59: Total Reward = 18.0\n",
            "Episode 60: Total Reward = 47.0\n",
            "Episode 61: Total Reward = 29.0\n",
            "Episode 62: Total Reward = 25.0\n",
            "Episode 63: Total Reward = 15.0\n",
            "Episode 64: Total Reward = 22.0\n",
            "Episode 65: Total Reward = 26.0\n",
            "Episode 66: Total Reward = 12.0\n",
            "Episode 67: Total Reward = 23.0\n",
            "Episode 68: Total Reward = 81.0\n",
            "Episode 69: Total Reward = 37.0\n",
            "Episode 70: Total Reward = 18.0\n",
            "Episode 71: Total Reward = 53.0\n",
            "Episode 72: Total Reward = 13.0\n",
            "Episode 73: Total Reward = 20.0\n",
            "Episode 74: Total Reward = 47.0\n",
            "Episode 75: Total Reward = 15.0\n",
            "Episode 76: Total Reward = 19.0\n",
            "Episode 77: Total Reward = 46.0\n",
            "Episode 78: Total Reward = 11.0\n",
            "Episode 79: Total Reward = 26.0\n",
            "Episode 80: Total Reward = 16.0\n",
            "Episode 81: Total Reward = 90.0\n",
            "Episode 82: Total Reward = 13.0\n",
            "Episode 83: Total Reward = 14.0\n",
            "Episode 84: Total Reward = 19.0\n",
            "Episode 85: Total Reward = 66.0\n",
            "Episode 86: Total Reward = 83.0\n",
            "Episode 87: Total Reward = 122.0\n",
            "Episode 88: Total Reward = 74.0\n",
            "Episode 89: Total Reward = 72.0\n",
            "Episode 90: Total Reward = 64.0\n",
            "Episode 91: Total Reward = 84.0\n",
            "Episode 92: Total Reward = 81.0\n",
            "Episode 93: Total Reward = 86.0\n",
            "Episode 94: Total Reward = 100.0\n",
            "Episode 95: Total Reward = 14.0\n",
            "Episode 96: Total Reward = 59.0\n",
            "Episode 97: Total Reward = 30.0\n",
            "Episode 98: Total Reward = 10.0\n",
            "Episode 99: Total Reward = 23.0\n",
            "Episode 100: Total Reward = 25.0\n",
            "Episode 101: Total Reward = 59.0\n",
            "Episode 102: Total Reward = 112.0\n",
            "Episode 103: Total Reward = 53.0\n",
            "Episode 104: Total Reward = 20.0\n",
            "Episode 105: Total Reward = 35.0\n",
            "Episode 106: Total Reward = 14.0\n",
            "Episode 107: Total Reward = 130.0\n",
            "Episode 108: Total Reward = 36.0\n",
            "Episode 109: Total Reward = 90.0\n",
            "Episode 110: Total Reward = 90.0\n",
            "Episode 111: Total Reward = 92.0\n",
            "Episode 112: Total Reward = 130.0\n",
            "Episode 113: Total Reward = 90.0\n",
            "Episode 114: Total Reward = 103.0\n",
            "Episode 115: Total Reward = 134.0\n",
            "Episode 116: Total Reward = 16.0\n",
            "Episode 117: Total Reward = 22.0\n",
            "Episode 118: Total Reward = 145.0\n",
            "Episode 119: Total Reward = 113.0\n",
            "Episode 120: Total Reward = 85.0\n",
            "Episode 121: Total Reward = 113.0\n",
            "Episode 122: Total Reward = 124.0\n",
            "Episode 123: Total Reward = 125.0\n",
            "Episode 124: Total Reward = 101.0\n",
            "Episode 125: Total Reward = 146.0\n",
            "Episode 126: Total Reward = 102.0\n",
            "Episode 127: Total Reward = 77.0\n",
            "Episode 128: Total Reward = 110.0\n",
            "Episode 129: Total Reward = 116.0\n",
            "Episode 130: Total Reward = 27.0\n",
            "Episode 131: Total Reward = 123.0\n",
            "Episode 132: Total Reward = 119.0\n",
            "Episode 133: Total Reward = 29.0\n",
            "Episode 134: Total Reward = 121.0\n",
            "Episode 135: Total Reward = 118.0\n",
            "Episode 136: Total Reward = 29.0\n",
            "Episode 137: Total Reward = 149.0\n",
            "Episode 138: Total Reward = 134.0\n",
            "Episode 139: Total Reward = 152.0\n",
            "Episode 140: Total Reward = 158.0\n",
            "Episode 141: Total Reward = 140.0\n",
            "Episode 142: Total Reward = 10.0\n",
            "Episode 143: Total Reward = 80.0\n",
            "Episode 144: Total Reward = 151.0\n",
            "Episode 145: Total Reward = 121.0\n",
            "Episode 146: Total Reward = 229.0\n",
            "Episode 147: Total Reward = 144.0\n",
            "Episode 148: Total Reward = 48.0\n",
            "Episode 149: Total Reward = 148.0\n",
            "Episode 150: Total Reward = 15.0\n",
            "Episode 151: Total Reward = 172.0\n",
            "Episode 152: Total Reward = 145.0\n",
            "Episode 153: Total Reward = 176.0\n",
            "Episode 154: Total Reward = 13.0\n",
            "Episode 155: Total Reward = 153.0\n",
            "Episode 156: Total Reward = 121.0\n",
            "Episode 157: Total Reward = 126.0\n",
            "Episode 158: Total Reward = 226.0\n",
            "Episode 159: Total Reward = 89.0\n",
            "Episode 160: Total Reward = 65.0\n",
            "Episode 161: Total Reward = 142.0\n",
            "Episode 162: Total Reward = 177.0\n",
            "Episode 163: Total Reward = 124.0\n",
            "Episode 164: Total Reward = 31.0\n",
            "Episode 165: Total Reward = 134.0\n",
            "Episode 166: Total Reward = 102.0\n",
            "Episode 167: Total Reward = 36.0\n",
            "Episode 168: Total Reward = 172.0\n",
            "Episode 169: Total Reward = 162.0\n",
            "Episode 170: Total Reward = 199.0\n",
            "Episode 171: Total Reward = 205.0\n",
            "Episode 172: Total Reward = 19.0\n",
            "Episode 173: Total Reward = 185.0\n",
            "Episode 174: Total Reward = 161.0\n",
            "Episode 175: Total Reward = 17.0\n",
            "Episode 176: Total Reward = 119.0\n",
            "Episode 177: Total Reward = 113.0\n",
            "Episode 178: Total Reward = 122.0\n",
            "Episode 179: Total Reward = 149.0\n",
            "Episode 180: Total Reward = 115.0\n",
            "Episode 181: Total Reward = 115.0\n",
            "Episode 182: Total Reward = 41.0\n",
            "Episode 183: Total Reward = 194.0\n",
            "Episode 184: Total Reward = 200.0\n",
            "Episode 185: Total Reward = 224.0\n",
            "Episode 186: Total Reward = 196.0\n",
            "Episode 187: Total Reward = 182.0\n",
            "Episode 188: Total Reward = 201.0\n",
            "Episode 189: Total Reward = 208.0\n",
            "Episode 190: Total Reward = 130.0\n",
            "Episode 191: Total Reward = 50.0\n",
            "Episode 192: Total Reward = 33.0\n",
            "Episode 193: Total Reward = 119.0\n",
            "Episode 194: Total Reward = 184.0\n",
            "Episode 195: Total Reward = 314.0\n",
            "Episode 196: Total Reward = 205.0\n",
            "Episode 197: Total Reward = 246.0\n",
            "Episode 198: Total Reward = 246.0\n",
            "Episode 199: Total Reward = 160.0\n",
            "Episode 200: Total Reward = 118.0\n",
            "Episode 201: Total Reward = 211.0\n",
            "Episode 202: Total Reward = 218.0\n",
            "Episode 203: Total Reward = 280.0\n",
            "Episode 204: Total Reward = 152.0\n",
            "Episode 205: Total Reward = 264.0\n",
            "Episode 206: Total Reward = 241.0\n",
            "Episode 207: Total Reward = 273.0\n",
            "Episode 208: Total Reward = 220.0\n",
            "Episode 209: Total Reward = 288.0\n",
            "Episode 210: Total Reward = 156.0\n",
            "Episode 211: Total Reward = 363.0\n",
            "Episode 212: Total Reward = 211.0\n",
            "Episode 213: Total Reward = 190.0\n",
            "Episode 214: Total Reward = 287.0\n",
            "Episode 215: Total Reward = 343.0\n",
            "Episode 216: Total Reward = 268.0\n",
            "Episode 217: Total Reward = 248.0\n",
            "Episode 218: Total Reward = 272.0\n",
            "Episode 219: Total Reward = 202.0\n",
            "Episode 220: Total Reward = 370.0\n",
            "Episode 221: Total Reward = 280.0\n",
            "Episode 222: Total Reward = 299.0\n",
            "Episode 223: Total Reward = 500.0\n",
            "Episode 224: Total Reward = 309.0\n",
            "Episode 225: Total Reward = 424.0\n",
            "Episode 226: Total Reward = 334.0\n",
            "Episode 227: Total Reward = 426.0\n",
            "Episode 228: Total Reward = 424.0\n",
            "Episode 229: Total Reward = 433.0\n",
            "Episode 230: Total Reward = 500.0\n",
            "Episode 231: Total Reward = 358.0\n",
            "Episode 232: Total Reward = 414.0\n",
            "Episode 233: Total Reward = 263.0\n",
            "Episode 234: Total Reward = 317.0\n",
            "Episode 235: Total Reward = 394.0\n",
            "Episode 236: Total Reward = 264.0\n",
            "Episode 237: Total Reward = 284.0\n",
            "Episode 238: Total Reward = 259.0\n",
            "Episode 239: Total Reward = 310.0\n",
            "Episode 240: Total Reward = 426.0\n",
            "Episode 241: Total Reward = 310.0\n",
            "Episode 242: Total Reward = 113.0\n",
            "Episode 243: Total Reward = 116.0\n",
            "Episode 244: Total Reward = 322.0\n",
            "Episode 245: Total Reward = 314.0\n",
            "Episode 246: Total Reward = 359.0\n",
            "Episode 247: Total Reward = 371.0\n",
            "Episode 248: Total Reward = 370.0\n",
            "Episode 249: Total Reward = 407.0\n",
            "Episode 250: Total Reward = 418.0\n",
            "Episode 251: Total Reward = 445.0\n",
            "Episode 252: Total Reward = 349.0\n",
            "Episode 253: Total Reward = 339.0\n",
            "Episode 254: Total Reward = 317.0\n",
            "Episode 255: Total Reward = 374.0\n",
            "Episode 256: Total Reward = 481.0\n",
            "Episode 257: Total Reward = 373.0\n",
            "Episode 258: Total Reward = 443.0\n",
            "Episode 259: Total Reward = 413.0\n",
            "Episode 260: Total Reward = 131.0\n",
            "Episode 261: Total Reward = 125.0\n",
            "Episode 262: Total Reward = 328.0\n",
            "Episode 263: Total Reward = 392.0\n",
            "Episode 264: Total Reward = 431.0\n",
            "Episode 265: Total Reward = 397.0\n",
            "Episode 266: Total Reward = 321.0\n",
            "Episode 267: Total Reward = 393.0\n",
            "Episode 268: Total Reward = 337.0\n",
            "Episode 269: Total Reward = 340.0\n",
            "Episode 270: Total Reward = 389.0\n",
            "Episode 271: Total Reward = 390.0\n",
            "Episode 272: Total Reward = 342.0\n",
            "Episode 273: Total Reward = 327.0\n",
            "Episode 274: Total Reward = 401.0\n",
            "Episode 275: Total Reward = 140.0\n",
            "Episode 276: Total Reward = 144.0\n",
            "Episode 277: Total Reward = 380.0\n",
            "Episode 278: Total Reward = 347.0\n",
            "Episode 279: Total Reward = 379.0\n",
            "Episode 280: Total Reward = 361.0\n",
            "Episode 281: Total Reward = 322.0\n",
            "Episode 282: Total Reward = 364.0\n",
            "Episode 283: Total Reward = 361.0\n",
            "Episode 284: Total Reward = 400.0\n",
            "Episode 285: Total Reward = 362.0\n",
            "Episode 286: Total Reward = 430.0\n",
            "Episode 287: Total Reward = 468.0\n",
            "Episode 288: Total Reward = 32.0\n",
            "Episode 289: Total Reward = 455.0\n",
            "Episode 290: Total Reward = 437.0\n",
            "Episode 291: Total Reward = 500.0\n",
            "Episode 292: Total Reward = 162.0\n",
            "Episode 293: Total Reward = 426.0\n",
            "Episode 294: Total Reward = 172.0\n",
            "Episode 295: Total Reward = 160.0\n",
            "Episode 296: Total Reward = 469.0\n",
            "Episode 297: Total Reward = 433.0\n",
            "Episode 298: Total Reward = 420.0\n",
            "Episode 299: Total Reward = 500.0\n",
            "Episode 300: Total Reward = 433.0\n",
            "Episode 301: Total Reward = 485.0\n",
            "Episode 302: Total Reward = 328.0\n",
            "Episode 303: Total Reward = 143.0\n",
            "Episode 304: Total Reward = 142.0\n",
            "Episode 305: Total Reward = 125.0\n",
            "Episode 306: Total Reward = 44.0\n",
            "Episode 307: Total Reward = 137.0\n",
            "Episode 308: Total Reward = 135.0\n",
            "Episode 309: Total Reward = 185.0\n",
            "Episode 310: Total Reward = 144.0\n",
            "Episode 311: Total Reward = 231.0\n",
            "Episode 312: Total Reward = 428.0\n",
            "Episode 313: Total Reward = 306.0\n",
            "Episode 314: Total Reward = 356.0\n",
            "Episode 315: Total Reward = 237.0\n",
            "Episode 316: Total Reward = 390.0\n",
            "Episode 317: Total Reward = 281.0\n",
            "Episode 318: Total Reward = 289.0\n",
            "Episode 319: Total Reward = 500.0\n",
            "Episode 320: Total Reward = 183.0\n",
            "Episode 321: Total Reward = 315.0\n",
            "Episode 322: Total Reward = 398.0\n",
            "Episode 323: Total Reward = 125.0\n",
            "Episode 324: Total Reward = 496.0\n",
            "Episode 325: Total Reward = 176.0\n",
            "Episode 326: Total Reward = 162.0\n",
            "Episode 327: Total Reward = 195.0\n",
            "Episode 328: Total Reward = 129.0\n",
            "Episode 329: Total Reward = 200.0\n",
            "Episode 330: Total Reward = 359.0\n",
            "Episode 331: Total Reward = 367.0\n",
            "Episode 332: Total Reward = 152.0\n",
            "Episode 333: Total Reward = 500.0\n",
            "Episode 334: Total Reward = 500.0\n",
            "Episode 335: Total Reward = 259.0\n",
            "Episode 336: Total Reward = 117.0\n",
            "Episode 337: Total Reward = 269.0\n",
            "Episode 338: Total Reward = 338.0\n",
            "Episode 339: Total Reward = 142.0\n",
            "Episode 340: Total Reward = 289.0\n",
            "Episode 341: Total Reward = 135.0\n",
            "Episode 342: Total Reward = 138.0\n",
            "Episode 343: Total Reward = 134.0\n",
            "Episode 344: Total Reward = 143.0\n",
            "Episode 345: Total Reward = 144.0\n",
            "Episode 346: Total Reward = 160.0\n",
            "Episode 347: Total Reward = 139.0\n",
            "Episode 348: Total Reward = 122.0\n",
            "Episode 349: Total Reward = 135.0\n",
            "Episode 350: Total Reward = 115.0\n",
            "Episode 351: Total Reward = 128.0\n",
            "Episode 352: Total Reward = 133.0\n",
            "Episode 353: Total Reward = 132.0\n",
            "Episode 354: Total Reward = 143.0\n",
            "Episode 355: Total Reward = 218.0\n",
            "Episode 356: Total Reward = 122.0\n",
            "Episode 357: Total Reward = 142.0\n",
            "Episode 358: Total Reward = 122.0\n",
            "Episode 359: Total Reward = 136.0\n",
            "Episode 360: Total Reward = 126.0\n",
            "Episode 361: Total Reward = 155.0\n",
            "Episode 362: Total Reward = 148.0\n",
            "Episode 363: Total Reward = 114.0\n",
            "Episode 364: Total Reward = 106.0\n",
            "Episode 365: Total Reward = 156.0\n",
            "Episode 366: Total Reward = 115.0\n",
            "Episode 367: Total Reward = 147.0\n",
            "Episode 368: Total Reward = 119.0\n",
            "Episode 369: Total Reward = 108.0\n",
            "Episode 370: Total Reward = 121.0\n",
            "Episode 371: Total Reward = 117.0\n",
            "Episode 372: Total Reward = 125.0\n",
            "Episode 373: Total Reward = 125.0\n",
            "Episode 374: Total Reward = 126.0\n",
            "Episode 375: Total Reward = 115.0\n",
            "Episode 376: Total Reward = 121.0\n",
            "Episode 377: Total Reward = 139.0\n",
            "Episode 378: Total Reward = 115.0\n",
            "Episode 379: Total Reward = 120.0\n",
            "Episode 380: Total Reward = 140.0\n",
            "Episode 381: Total Reward = 117.0\n",
            "Episode 382: Total Reward = 110.0\n",
            "Episode 383: Total Reward = 110.0\n",
            "Episode 384: Total Reward = 110.0\n",
            "Episode 385: Total Reward = 103.0\n",
            "Episode 386: Total Reward = 115.0\n",
            "Episode 387: Total Reward = 105.0\n",
            "Episode 388: Total Reward = 108.0\n",
            "Episode 389: Total Reward = 105.0\n",
            "Episode 390: Total Reward = 106.0\n",
            "Episode 391: Total Reward = 113.0\n",
            "Episode 392: Total Reward = 107.0\n",
            "Episode 393: Total Reward = 112.0\n",
            "Episode 394: Total Reward = 109.0\n",
            "Episode 395: Total Reward = 109.0\n",
            "Episode 396: Total Reward = 111.0\n",
            "Episode 397: Total Reward = 106.0\n",
            "Episode 398: Total Reward = 110.0\n",
            "Episode 399: Total Reward = 109.0\n",
            "Episode 400: Total Reward = 106.0\n",
            "Episode 401: Total Reward = 25.0\n",
            "Episode 402: Total Reward = 107.0\n",
            "Episode 403: Total Reward = 108.0\n",
            "Episode 404: Total Reward = 106.0\n",
            "Episode 405: Total Reward = 113.0\n",
            "Episode 406: Total Reward = 117.0\n",
            "Episode 407: Total Reward = 110.0\n",
            "Episode 408: Total Reward = 106.0\n",
            "Episode 409: Total Reward = 112.0\n",
            "Episode 410: Total Reward = 105.0\n",
            "Episode 411: Total Reward = 108.0\n",
            "Episode 412: Total Reward = 107.0\n",
            "Episode 413: Total Reward = 103.0\n",
            "Episode 414: Total Reward = 112.0\n",
            "Episode 415: Total Reward = 115.0\n",
            "Episode 416: Total Reward = 113.0\n",
            "Episode 417: Total Reward = 113.0\n",
            "Episode 418: Total Reward = 104.0\n",
            "Episode 419: Total Reward = 115.0\n",
            "Episode 420: Total Reward = 115.0\n",
            "Episode 421: Total Reward = 107.0\n",
            "Episode 422: Total Reward = 108.0\n",
            "Episode 423: Total Reward = 119.0\n",
            "Episode 424: Total Reward = 114.0\n",
            "Episode 425: Total Reward = 120.0\n",
            "Episode 426: Total Reward = 110.0\n",
            "Episode 427: Total Reward = 120.0\n",
            "Episode 428: Total Reward = 117.0\n",
            "Episode 429: Total Reward = 121.0\n",
            "Episode 430: Total Reward = 111.0\n",
            "Episode 431: Total Reward = 120.0\n",
            "Episode 432: Total Reward = 125.0\n",
            "Episode 433: Total Reward = 115.0\n",
            "Episode 434: Total Reward = 116.0\n",
            "Episode 435: Total Reward = 120.0\n",
            "Episode 436: Total Reward = 112.0\n",
            "Episode 437: Total Reward = 114.0\n",
            "Episode 438: Total Reward = 119.0\n",
            "Episode 439: Total Reward = 117.0\n",
            "Episode 440: Total Reward = 112.0\n",
            "Episode 441: Total Reward = 123.0\n",
            "Episode 442: Total Reward = 112.0\n",
            "Episode 443: Total Reward = 107.0\n",
            "Episode 444: Total Reward = 115.0\n",
            "Episode 445: Total Reward = 112.0\n",
            "Episode 446: Total Reward = 106.0\n",
            "Episode 447: Total Reward = 129.0\n",
            "Episode 448: Total Reward = 118.0\n",
            "Episode 449: Total Reward = 113.0\n",
            "Episode 450: Total Reward = 113.0\n",
            "Episode 451: Total Reward = 110.0\n",
            "Episode 452: Total Reward = 110.0\n",
            "Episode 453: Total Reward = 117.0\n",
            "Episode 454: Total Reward = 119.0\n",
            "Episode 455: Total Reward = 110.0\n",
            "Episode 456: Total Reward = 108.0\n",
            "Episode 457: Total Reward = 111.0\n",
            "Episode 458: Total Reward = 110.0\n",
            "Episode 459: Total Reward = 114.0\n",
            "Episode 460: Total Reward = 111.0\n",
            "Episode 461: Total Reward = 108.0\n",
            "Episode 462: Total Reward = 135.0\n",
            "Episode 463: Total Reward = 146.0\n",
            "Episode 464: Total Reward = 149.0\n",
            "Episode 465: Total Reward = 156.0\n",
            "Episode 466: Total Reward = 135.0\n",
            "Episode 467: Total Reward = 161.0\n",
            "Episode 468: Total Reward = 186.0\n",
            "Episode 469: Total Reward = 189.0\n",
            "Episode 470: Total Reward = 179.0\n",
            "Episode 471: Total Reward = 138.0\n",
            "Episode 472: Total Reward = 133.0\n",
            "Episode 473: Total Reward = 141.0\n",
            "Episode 474: Total Reward = 150.0\n",
            "Episode 475: Total Reward = 137.0\n",
            "Episode 476: Total Reward = 151.0\n",
            "Episode 477: Total Reward = 130.0\n",
            "Episode 478: Total Reward = 145.0\n",
            "Episode 479: Total Reward = 149.0\n",
            "Episode 480: Total Reward = 137.0\n",
            "Episode 481: Total Reward = 147.0\n",
            "Episode 482: Total Reward = 155.0\n",
            "Episode 483: Total Reward = 174.0\n",
            "Episode 484: Total Reward = 187.0\n",
            "Episode 485: Total Reward = 156.0\n",
            "Episode 486: Total Reward = 156.0\n",
            "Episode 487: Total Reward = 185.0\n",
            "Episode 488: Total Reward = 178.0\n",
            "Episode 489: Total Reward = 208.0\n",
            "Episode 490: Total Reward = 162.0\n",
            "Episode 491: Total Reward = 173.0\n",
            "Episode 492: Total Reward = 159.0\n",
            "Episode 493: Total Reward = 172.0\n",
            "Episode 494: Total Reward = 155.0\n",
            "Episode 495: Total Reward = 177.0\n",
            "Episode 496: Total Reward = 186.0\n",
            "Episode 497: Total Reward = 162.0\n",
            "Episode 498: Total Reward = 161.0\n",
            "Episode 499: Total Reward = 178.0\n",
            "Episode 500: Total Reward = 169.0\n",
            "Episode 501: Total Reward = 214.0\n",
            "Episode 502: Total Reward = 177.0\n",
            "Episode 503: Total Reward = 156.0\n",
            "Episode 504: Total Reward = 164.0\n",
            "Episode 505: Total Reward = 196.0\n",
            "Episode 506: Total Reward = 156.0\n",
            "Episode 507: Total Reward = 151.0\n",
            "Episode 508: Total Reward = 142.0\n",
            "Episode 509: Total Reward = 150.0\n",
            "Episode 510: Total Reward = 150.0\n",
            "Episode 511: Total Reward = 176.0\n",
            "Episode 512: Total Reward = 170.0\n",
            "Episode 513: Total Reward = 163.0\n",
            "Episode 514: Total Reward = 158.0\n",
            "Episode 515: Total Reward = 168.0\n",
            "Episode 516: Total Reward = 145.0\n",
            "Episode 517: Total Reward = 147.0\n",
            "Episode 518: Total Reward = 148.0\n",
            "Episode 519: Total Reward = 154.0\n",
            "Episode 520: Total Reward = 157.0\n",
            "Episode 521: Total Reward = 160.0\n",
            "Episode 522: Total Reward = 154.0\n",
            "Episode 523: Total Reward = 174.0\n",
            "Episode 524: Total Reward = 187.0\n",
            "Episode 525: Total Reward = 184.0\n",
            "Episode 526: Total Reward = 163.0\n",
            "Episode 527: Total Reward = 167.0\n",
            "Episode 528: Total Reward = 190.0\n",
            "Episode 529: Total Reward = 189.0\n",
            "Episode 530: Total Reward = 223.0\n",
            "Episode 531: Total Reward = 198.0\n",
            "Episode 532: Total Reward = 172.0\n",
            "Episode 533: Total Reward = 169.0\n",
            "Episode 534: Total Reward = 194.0\n",
            "Episode 535: Total Reward = 168.0\n",
            "Episode 536: Total Reward = 177.0\n",
            "Episode 537: Total Reward = 203.0\n",
            "Episode 538: Total Reward = 177.0\n",
            "Episode 539: Total Reward = 269.0\n",
            "Episode 540: Total Reward = 174.0\n",
            "Episode 541: Total Reward = 227.0\n",
            "Episode 542: Total Reward = 265.0\n",
            "Episode 543: Total Reward = 267.0\n",
            "Episode 544: Total Reward = 279.0\n",
            "Episode 545: Total Reward = 210.0\n",
            "Episode 546: Total Reward = 283.0\n",
            "Episode 547: Total Reward = 271.0\n",
            "Episode 548: Total Reward = 269.0\n",
            "Episode 549: Total Reward = 266.0\n",
            "Episode 550: Total Reward = 262.0\n",
            "Episode 551: Total Reward = 263.0\n",
            "Episode 552: Total Reward = 211.0\n",
            "Episode 553: Total Reward = 384.0\n",
            "Episode 554: Total Reward = 252.0\n",
            "Episode 555: Total Reward = 268.0\n",
            "Episode 556: Total Reward = 360.0\n",
            "Episode 557: Total Reward = 190.0\n",
            "Episode 558: Total Reward = 213.0\n",
            "Episode 559: Total Reward = 235.0\n",
            "Episode 560: Total Reward = 279.0\n",
            "Episode 561: Total Reward = 250.0\n",
            "Episode 562: Total Reward = 192.0\n",
            "Episode 563: Total Reward = 205.0\n",
            "Episode 564: Total Reward = 197.0\n",
            "Episode 565: Total Reward = 212.0\n",
            "Episode 566: Total Reward = 217.0\n",
            "Episode 567: Total Reward = 211.0\n",
            "Episode 568: Total Reward = 238.0\n",
            "Episode 569: Total Reward = 213.0\n",
            "Episode 570: Total Reward = 201.0\n",
            "Episode 571: Total Reward = 253.0\n",
            "Episode 572: Total Reward = 216.0\n",
            "Episode 573: Total Reward = 221.0\n",
            "Episode 574: Total Reward = 219.0\n",
            "Episode 575: Total Reward = 230.0\n",
            "Episode 576: Total Reward = 254.0\n",
            "Episode 577: Total Reward = 233.0\n",
            "Episode 578: Total Reward = 233.0\n",
            "Episode 579: Total Reward = 265.0\n",
            "Episode 580: Total Reward = 240.0\n",
            "Episode 581: Total Reward = 247.0\n",
            "Episode 582: Total Reward = 192.0\n",
            "Episode 583: Total Reward = 210.0\n",
            "Episode 584: Total Reward = 204.0\n",
            "Episode 585: Total Reward = 182.0\n",
            "Episode 586: Total Reward = 193.0\n",
            "Episode 587: Total Reward = 174.0\n",
            "Episode 588: Total Reward = 186.0\n",
            "Episode 589: Total Reward = 196.0\n",
            "Episode 590: Total Reward = 204.0\n",
            "Episode 591: Total Reward = 153.0\n",
            "Episode 592: Total Reward = 182.0\n",
            "Episode 593: Total Reward = 159.0\n",
            "Episode 594: Total Reward = 201.0\n",
            "Episode 595: Total Reward = 157.0\n",
            "Episode 596: Total Reward = 164.0\n",
            "Episode 597: Total Reward = 178.0\n",
            "Episode 598: Total Reward = 172.0\n",
            "Episode 599: Total Reward = 177.0\n",
            "Episode 600: Total Reward = 158.0\n",
            "Episode 601: Total Reward = 199.0\n",
            "Episode 602: Total Reward = 200.0\n",
            "Episode 603: Total Reward = 198.0\n",
            "Episode 604: Total Reward = 190.0\n",
            "Episode 605: Total Reward = 310.0\n",
            "Episode 606: Total Reward = 267.0\n",
            "Episode 607: Total Reward = 216.0\n",
            "Episode 608: Total Reward = 231.0\n",
            "Episode 609: Total Reward = 338.0\n",
            "Episode 610: Total Reward = 247.0\n",
            "Episode 611: Total Reward = 263.0\n",
            "Episode 612: Total Reward = 482.0\n",
            "Episode 613: Total Reward = 361.0\n",
            "Episode 614: Total Reward = 222.0\n",
            "Episode 615: Total Reward = 295.0\n",
            "Episode 616: Total Reward = 276.0\n",
            "Episode 617: Total Reward = 220.0\n",
            "Episode 618: Total Reward = 210.0\n",
            "Episode 619: Total Reward = 306.0\n",
            "Episode 620: Total Reward = 362.0\n",
            "Episode 621: Total Reward = 353.0\n",
            "Episode 622: Total Reward = 183.0\n",
            "Episode 623: Total Reward = 178.0\n",
            "Episode 624: Total Reward = 190.0\n",
            "Episode 625: Total Reward = 181.0\n",
            "Episode 626: Total Reward = 189.0\n",
            "Episode 627: Total Reward = 175.0\n",
            "Episode 628: Total Reward = 200.0\n",
            "Episode 629: Total Reward = 189.0\n",
            "Episode 630: Total Reward = 188.0\n",
            "Episode 631: Total Reward = 173.0\n",
            "Episode 632: Total Reward = 183.0\n",
            "Episode 633: Total Reward = 181.0\n",
            "Episode 634: Total Reward = 176.0\n",
            "Episode 635: Total Reward = 172.0\n",
            "Episode 636: Total Reward = 172.0\n",
            "Episode 637: Total Reward = 175.0\n",
            "Episode 638: Total Reward = 186.0\n",
            "Episode 639: Total Reward = 174.0\n",
            "Episode 640: Total Reward = 150.0\n",
            "Episode 641: Total Reward = 189.0\n",
            "Episode 642: Total Reward = 303.0\n",
            "Episode 643: Total Reward = 265.0\n",
            "Episode 644: Total Reward = 225.0\n",
            "Episode 645: Total Reward = 262.0\n",
            "Episode 646: Total Reward = 210.0\n",
            "Episode 647: Total Reward = 211.0\n",
            "Episode 648: Total Reward = 216.0\n",
            "Episode 649: Total Reward = 198.0\n",
            "Episode 650: Total Reward = 189.0\n",
            "Episode 651: Total Reward = 265.0\n",
            "Episode 652: Total Reward = 210.0\n",
            "Episode 653: Total Reward = 214.0\n",
            "Episode 654: Total Reward = 217.0\n",
            "Episode 655: Total Reward = 248.0\n",
            "Episode 656: Total Reward = 230.0\n",
            "Episode 657: Total Reward = 202.0\n",
            "Episode 658: Total Reward = 185.0\n",
            "Episode 659: Total Reward = 204.0\n",
            "Episode 660: Total Reward = 198.0\n",
            "Episode 661: Total Reward = 208.0\n",
            "Episode 662: Total Reward = 246.0\n",
            "Episode 663: Total Reward = 215.0\n",
            "Episode 664: Total Reward = 206.0\n",
            "Episode 665: Total Reward = 307.0\n",
            "Episode 666: Total Reward = 184.0\n",
            "Episode 667: Total Reward = 222.0\n",
            "Episode 668: Total Reward = 207.0\n",
            "Episode 669: Total Reward = 222.0\n",
            "Episode 670: Total Reward = 193.0\n",
            "Episode 671: Total Reward = 223.0\n",
            "Episode 672: Total Reward = 206.0\n",
            "Episode 673: Total Reward = 201.0\n",
            "Episode 674: Total Reward = 176.0\n",
            "Episode 675: Total Reward = 217.0\n",
            "Episode 676: Total Reward = 184.0\n",
            "Episode 677: Total Reward = 223.0\n",
            "Episode 678: Total Reward = 192.0\n",
            "Episode 679: Total Reward = 207.0\n",
            "Episode 680: Total Reward = 213.0\n",
            "Episode 681: Total Reward = 254.0\n",
            "Episode 682: Total Reward = 217.0\n",
            "Episode 683: Total Reward = 344.0\n",
            "Episode 684: Total Reward = 298.0\n",
            "Episode 685: Total Reward = 243.0\n",
            "Episode 686: Total Reward = 300.0\n",
            "Episode 687: Total Reward = 325.0\n",
            "Episode 688: Total Reward = 290.0\n",
            "Episode 689: Total Reward = 221.0\n",
            "Episode 690: Total Reward = 238.0\n",
            "Episode 691: Total Reward = 221.0\n",
            "Episode 692: Total Reward = 263.0\n",
            "Episode 693: Total Reward = 282.0\n",
            "Episode 694: Total Reward = 250.0\n",
            "Episode 695: Total Reward = 305.0\n",
            "Episode 696: Total Reward = 265.0\n",
            "Episode 697: Total Reward = 262.0\n",
            "Episode 698: Total Reward = 226.0\n",
            "Episode 699: Total Reward = 298.0\n",
            "Episode 700: Total Reward = 234.0\n",
            "Episode 701: Total Reward = 261.0\n",
            "Episode 702: Total Reward = 231.0\n",
            "Episode 703: Total Reward = 339.0\n",
            "Episode 704: Total Reward = 329.0\n",
            "Episode 705: Total Reward = 500.0\n",
            "Episode 706: Total Reward = 189.0\n",
            "Episode 707: Total Reward = 194.0\n",
            "Episode 708: Total Reward = 179.0\n",
            "Episode 709: Total Reward = 184.0\n",
            "Episode 710: Total Reward = 217.0\n",
            "Episode 711: Total Reward = 500.0\n",
            "Episode 712: Total Reward = 500.0\n",
            "Episode 713: Total Reward = 500.0\n",
            "Episode 714: Total Reward = 500.0\n",
            "Episode 715: Total Reward = 253.0\n",
            "Episode 716: Total Reward = 316.0\n",
            "Episode 717: Total Reward = 179.0\n",
            "Episode 718: Total Reward = 197.0\n",
            "Episode 719: Total Reward = 184.0\n",
            "Episode 720: Total Reward = 500.0\n",
            "Episode 721: Total Reward = 210.0\n",
            "Episode 722: Total Reward = 267.0\n",
            "Episode 723: Total Reward = 198.0\n",
            "Episode 724: Total Reward = 170.0\n",
            "Episode 725: Total Reward = 194.0\n",
            "Episode 726: Total Reward = 184.0\n",
            "Episode 727: Total Reward = 191.0\n",
            "Episode 728: Total Reward = 171.0\n",
            "Episode 729: Total Reward = 193.0\n",
            "Episode 730: Total Reward = 178.0\n",
            "Episode 731: Total Reward = 196.0\n",
            "Episode 732: Total Reward = 199.0\n",
            "Episode 733: Total Reward = 185.0\n",
            "Episode 734: Total Reward = 188.0\n",
            "Episode 735: Total Reward = 176.0\n",
            "Episode 736: Total Reward = 213.0\n",
            "Episode 737: Total Reward = 175.0\n",
            "Episode 738: Total Reward = 175.0\n",
            "Episode 739: Total Reward = 176.0\n",
            "Episode 740: Total Reward = 248.0\n",
            "Episode 741: Total Reward = 197.0\n",
            "Episode 742: Total Reward = 189.0\n",
            "Episode 743: Total Reward = 459.0\n",
            "Episode 744: Total Reward = 374.0\n",
            "Episode 745: Total Reward = 418.0\n",
            "Episode 746: Total Reward = 414.0\n",
            "Episode 747: Total Reward = 408.0\n",
            "Episode 748: Total Reward = 241.0\n",
            "Episode 749: Total Reward = 303.0\n",
            "Episode 750: Total Reward = 388.0\n",
            "Episode 751: Total Reward = 380.0\n",
            "Episode 752: Total Reward = 348.0\n",
            "Episode 753: Total Reward = 500.0\n",
            "Episode 754: Total Reward = 244.0\n",
            "Episode 755: Total Reward = 500.0\n",
            "Episode 756: Total Reward = 262.0\n",
            "Episode 757: Total Reward = 435.0\n",
            "Episode 758: Total Reward = 372.0\n",
            "Episode 759: Total Reward = 500.0\n",
            "Episode 760: Total Reward = 242.0\n",
            "Episode 761: Total Reward = 500.0\n",
            "Episode 762: Total Reward = 381.0\n",
            "Episode 763: Total Reward = 297.0\n",
            "Episode 764: Total Reward = 299.0\n",
            "Episode 765: Total Reward = 439.0\n",
            "Episode 766: Total Reward = 309.0\n",
            "Episode 767: Total Reward = 404.0\n",
            "Episode 768: Total Reward = 394.0\n",
            "Episode 769: Total Reward = 500.0\n",
            "Episode 770: Total Reward = 341.0\n",
            "Episode 771: Total Reward = 362.0\n",
            "Episode 772: Total Reward = 381.0\n",
            "Episode 773: Total Reward = 346.0\n",
            "Episode 774: Total Reward = 429.0\n",
            "Episode 775: Total Reward = 328.0\n",
            "Episode 776: Total Reward = 363.0\n",
            "Episode 777: Total Reward = 318.0\n",
            "Episode 778: Total Reward = 339.0\n",
            "Episode 779: Total Reward = 327.0\n",
            "Episode 780: Total Reward = 500.0\n",
            "Episode 781: Total Reward = 336.0\n",
            "Episode 782: Total Reward = 378.0\n",
            "Episode 783: Total Reward = 266.0\n",
            "Episode 784: Total Reward = 284.0\n",
            "Episode 785: Total Reward = 373.0\n",
            "Episode 786: Total Reward = 247.0\n",
            "Episode 787: Total Reward = 258.0\n",
            "Episode 788: Total Reward = 257.0\n",
            "Episode 789: Total Reward = 258.0\n",
            "Episode 790: Total Reward = 176.0\n",
            "Episode 791: Total Reward = 222.0\n",
            "Episode 792: Total Reward = 208.0\n",
            "Episode 793: Total Reward = 253.0\n",
            "Episode 794: Total Reward = 279.0\n",
            "Episode 795: Total Reward = 229.0\n",
            "Episode 796: Total Reward = 267.0\n",
            "Episode 797: Total Reward = 249.0\n",
            "Episode 798: Total Reward = 241.0\n",
            "Episode 799: Total Reward = 238.0\n",
            "Episode 800: Total Reward = 238.0\n",
            "Episode 801: Total Reward = 312.0\n",
            "Episode 802: Total Reward = 311.0\n",
            "Episode 803: Total Reward = 395.0\n",
            "Episode 804: Total Reward = 322.0\n",
            "Episode 805: Total Reward = 296.0\n",
            "Episode 806: Total Reward = 311.0\n",
            "Episode 807: Total Reward = 500.0\n",
            "Episode 808: Total Reward = 310.0\n",
            "Episode 809: Total Reward = 404.0\n",
            "Episode 810: Total Reward = 344.0\n",
            "Episode 811: Total Reward = 334.0\n",
            "Episode 812: Total Reward = 400.0\n",
            "Episode 813: Total Reward = 367.0\n",
            "Episode 814: Total Reward = 262.0\n",
            "Episode 815: Total Reward = 271.0\n",
            "Episode 816: Total Reward = 422.0\n",
            "Episode 817: Total Reward = 318.0\n",
            "Episode 818: Total Reward = 323.0\n",
            "Episode 819: Total Reward = 321.0\n",
            "Episode 820: Total Reward = 287.0\n",
            "Episode 821: Total Reward = 298.0\n",
            "Episode 822: Total Reward = 500.0\n",
            "Episode 823: Total Reward = 500.0\n",
            "Episode 824: Total Reward = 268.0\n",
            "Episode 825: Total Reward = 500.0\n",
            "Episode 826: Total Reward = 500.0\n",
            "Episode 827: Total Reward = 295.0\n",
            "Episode 828: Total Reward = 500.0\n",
            "Episode 829: Total Reward = 395.0\n",
            "Episode 830: Total Reward = 500.0\n",
            "Episode 831: Total Reward = 500.0\n",
            "Episode 832: Total Reward = 373.0\n",
            "Episode 833: Total Reward = 485.0\n",
            "Episode 834: Total Reward = 329.0\n",
            "Episode 835: Total Reward = 500.0\n",
            "Episode 836: Total Reward = 347.0\n",
            "Episode 837: Total Reward = 341.0\n",
            "Episode 838: Total Reward = 500.0\n",
            "Episode 839: Total Reward = 484.0\n",
            "Episode 840: Total Reward = 500.0\n",
            "Episode 841: Total Reward = 500.0\n",
            "Episode 842: Total Reward = 500.0\n",
            "Episode 843: Total Reward = 500.0\n",
            "Episode 844: Total Reward = 500.0\n",
            "Episode 845: Total Reward = 460.0\n",
            "Episode 846: Total Reward = 368.0\n",
            "Episode 847: Total Reward = 302.0\n",
            "Episode 848: Total Reward = 237.0\n",
            "Episode 849: Total Reward = 485.0\n",
            "Episode 850: Total Reward = 350.0\n",
            "Episode 851: Total Reward = 388.0\n",
            "Episode 852: Total Reward = 320.0\n",
            "Episode 853: Total Reward = 332.0\n",
            "Episode 854: Total Reward = 268.0\n",
            "Episode 855: Total Reward = 254.0\n",
            "Episode 856: Total Reward = 308.0\n",
            "Episode 857: Total Reward = 409.0\n",
            "Episode 858: Total Reward = 364.0\n",
            "Episode 859: Total Reward = 404.0\n",
            "Episode 860: Total Reward = 261.0\n",
            "Episode 861: Total Reward = 257.0\n",
            "Episode 862: Total Reward = 411.0\n",
            "Episode 863: Total Reward = 302.0\n",
            "Episode 864: Total Reward = 295.0\n",
            "Episode 865: Total Reward = 316.0\n",
            "Episode 866: Total Reward = 272.0\n",
            "Episode 867: Total Reward = 255.0\n",
            "Episode 868: Total Reward = 288.0\n",
            "Episode 869: Total Reward = 291.0\n",
            "Episode 870: Total Reward = 236.0\n",
            "Episode 871: Total Reward = 255.0\n",
            "Episode 872: Total Reward = 301.0\n",
            "Episode 873: Total Reward = 250.0\n",
            "Episode 874: Total Reward = 296.0\n",
            "Episode 875: Total Reward = 285.0\n",
            "Episode 876: Total Reward = 331.0\n",
            "Episode 877: Total Reward = 299.0\n",
            "Episode 878: Total Reward = 273.0\n",
            "Episode 879: Total Reward = 277.0\n",
            "Episode 880: Total Reward = 290.0\n",
            "Episode 881: Total Reward = 268.0\n",
            "Episode 882: Total Reward = 324.0\n",
            "Episode 883: Total Reward = 500.0\n",
            "Episode 884: Total Reward = 500.0\n",
            "Episode 885: Total Reward = 269.0\n",
            "Episode 886: Total Reward = 422.0\n",
            "Episode 887: Total Reward = 363.0\n",
            "Episode 888: Total Reward = 312.0\n",
            "Episode 889: Total Reward = 320.0\n",
            "Episode 890: Total Reward = 377.0\n",
            "Episode 891: Total Reward = 466.0\n",
            "Episode 892: Total Reward = 364.0\n",
            "Episode 893: Total Reward = 500.0\n",
            "Episode 894: Total Reward = 309.0\n",
            "Episode 895: Total Reward = 283.0\n",
            "Episode 896: Total Reward = 413.0\n",
            "Episode 897: Total Reward = 444.0\n",
            "Episode 898: Total Reward = 500.0\n",
            "Episode 899: Total Reward = 387.0\n",
            "Episode 900: Total Reward = 381.0\n",
            "Episode 901: Total Reward = 500.0\n",
            "Episode 902: Total Reward = 317.0\n",
            "Episode 903: Total Reward = 413.0\n",
            "Episode 904: Total Reward = 302.0\n",
            "Episode 905: Total Reward = 500.0\n",
            "Episode 906: Total Reward = 292.0\n",
            "Episode 907: Total Reward = 287.0\n",
            "Episode 908: Total Reward = 500.0\n",
            "Episode 909: Total Reward = 500.0\n",
            "Episode 910: Total Reward = 500.0\n",
            "Episode 911: Total Reward = 458.0\n",
            "Episode 912: Total Reward = 274.0\n",
            "Episode 913: Total Reward = 286.0\n",
            "Episode 914: Total Reward = 373.0\n",
            "Episode 915: Total Reward = 500.0\n",
            "Episode 916: Total Reward = 500.0\n",
            "Episode 917: Total Reward = 300.0\n",
            "Episode 918: Total Reward = 142.0\n",
            "Episode 919: Total Reward = 62.0\n",
            "Episode 920: Total Reward = 273.0\n",
            "Episode 921: Total Reward = 384.0\n",
            "Episode 922: Total Reward = 318.0\n",
            "Episode 923: Total Reward = 289.0\n",
            "Episode 924: Total Reward = 250.0\n",
            "Episode 925: Total Reward = 287.0\n",
            "Episode 926: Total Reward = 240.0\n",
            "Episode 927: Total Reward = 284.0\n",
            "Episode 928: Total Reward = 276.0\n",
            "Episode 929: Total Reward = 270.0\n",
            "Episode 930: Total Reward = 282.0\n",
            "Episode 931: Total Reward = 358.0\n",
            "Episode 932: Total Reward = 438.0\n",
            "Episode 933: Total Reward = 294.0\n",
            "Episode 934: Total Reward = 285.0\n",
            "Episode 935: Total Reward = 297.0\n",
            "Episode 936: Total Reward = 266.0\n",
            "Episode 937: Total Reward = 359.0\n",
            "Episode 938: Total Reward = 284.0\n",
            "Episode 939: Total Reward = 260.0\n",
            "Episode 940: Total Reward = 212.0\n",
            "Episode 941: Total Reward = 190.0\n",
            "Episode 942: Total Reward = 365.0\n",
            "Episode 943: Total Reward = 337.0\n",
            "Episode 944: Total Reward = 361.0\n",
            "Episode 945: Total Reward = 470.0\n",
            "Episode 946: Total Reward = 403.0\n",
            "Episode 947: Total Reward = 491.0\n",
            "Episode 948: Total Reward = 451.0\n",
            "Episode 949: Total Reward = 372.0\n",
            "Episode 950: Total Reward = 404.0\n",
            "Episode 951: Total Reward = 384.0\n",
            "Episode 952: Total Reward = 402.0\n",
            "Episode 953: Total Reward = 324.0\n",
            "Episode 954: Total Reward = 320.0\n",
            "Episode 955: Total Reward = 344.0\n",
            "Episode 956: Total Reward = 344.0\n",
            "Episode 957: Total Reward = 327.0\n",
            "Episode 958: Total Reward = 424.0\n",
            "Episode 959: Total Reward = 500.0\n",
            "Episode 960: Total Reward = 458.0\n",
            "Episode 961: Total Reward = 334.0\n",
            "Episode 962: Total Reward = 487.0\n",
            "Episode 963: Total Reward = 483.0\n",
            "Episode 964: Total Reward = 465.0\n",
            "Episode 965: Total Reward = 500.0\n",
            "Episode 966: Total Reward = 500.0\n",
            "Episode 967: Total Reward = 500.0\n",
            "Episode 968: Total Reward = 440.0\n",
            "Episode 969: Total Reward = 418.0\n",
            "Episode 970: Total Reward = 409.0\n",
            "Episode 971: Total Reward = 462.0\n",
            "Episode 972: Total Reward = 494.0\n",
            "Episode 973: Total Reward = 401.0\n",
            "Episode 974: Total Reward = 500.0\n",
            "Episode 975: Total Reward = 469.0\n",
            "Episode 976: Total Reward = 414.0\n",
            "Episode 977: Total Reward = 500.0\n",
            "Episode 978: Total Reward = 466.0\n",
            "Episode 979: Total Reward = 462.0\n",
            "Episode 980: Total Reward = 460.0\n",
            "Episode 981: Total Reward = 500.0\n",
            "Episode 982: Total Reward = 395.0\n",
            "Episode 983: Total Reward = 492.0\n",
            "Episode 984: Total Reward = 494.0\n",
            "Episode 985: Total Reward = 500.0\n",
            "Episode 986: Total Reward = 500.0\n",
            "Episode 987: Total Reward = 316.0\n",
            "Episode 988: Total Reward = 500.0\n",
            "Episode 989: Total Reward = 315.0\n",
            "Episode 990: Total Reward = 500.0\n",
            "Episode 991: Total Reward = 470.0\n",
            "Episode 992: Total Reward = 500.0\n",
            "Episode 993: Total Reward = 415.0\n",
            "Episode 994: Total Reward = 320.0\n",
            "Episode 995: Total Reward = 286.0\n",
            "Episode 996: Total Reward = 500.0\n",
            "Episode 997: Total Reward = 478.0\n",
            "Episode 998: Total Reward = 365.0\n",
            "Episode 999: Total Reward = 428.0\n",
            "Episode 1000: Total Reward = 500.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the policy network\n",
        "torch.save(policy_net.state_dict(), \"cartpole_policy_net.pth\")\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# Download the file to your local machine\n",
        "files.download(\"cartpole_policy_net.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "rYweH2UMzenn",
        "outputId": "a5e76b6a-2259-49a0-9e1c-7af168fc9844"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3003e1d7-a5c5-46f0-badb-c7cf223d9a55\", \"cartpole_policy_net.pth\", 177842)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}